import os
import cv2
from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
import math
import pyttsx3
import queue
import threading
import time
import io
from contextlib import redirect_stdout

# Suppress TensorFlow/Keras logs (this removes the output like 1/1 [==============================] - 0s 32ms/step)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# Initialize webcam and hand detector
cap = cv2.VideoCapture(0)  # Start the webcam feed
detector = HandDetector(maxHands=2)  # Detect up to two hands
classifier = Classifier("Model/keras_model.h5", "Model/labels.txt")  # Load model and labels
offset = 20  # Padding around the hand to create space
imgSize = 300  # Desired image size to feed to the model
labels = ["Hello", "How", "I", "I love you", "no", "peace", "Thankyou", "yes", "you"]  # Custom labels for gestures

# Initialize text-to-speech engine
engine = pyttsx3.init()

# Create a queue to manage speech requests
speech_queue = queue.Queue()

# Function for text-to-speech (runs in the main thread)
def speak_label():
    while True:
        label = speech_queue.get()  # Wait until a label is added to the queue
        if label == "STOP":
            break  # Stop the thread if the "STOP" signal is received
        engine.say(label)
        engine.runAndWait()

# Start the text-to-speech in a separate thread
speech_thread = threading.Thread(target=speak_label, daemon=True)
speech_thread.start()

# To store the previous predicted label to avoid saying the same word repeatedly
previous_labels = [None, None]  # One for each hand
last_speech_times = [0, 0]  # Timestamp for when the last speech was spoken for each hand
speech_delay = 1  # Wait time of 1 second after speaking before allowing the next speech
last_gesture_times = [0, 0]  # Timestamp for when the gesture last changed for each hand
gesture_delay = 0.5  # Delay to avoid repeating the gesture too quickly

# Function to suppress Keras output during prediction
def predict_without_output(img):
    f = io.StringIO()  # Create a string buffer to redirect output
    with redirect_stdout(f):  # Redirect stdout to the string buffer
        prediction, index = classifier.getPrediction(img, draw=False)  # Call the prediction method
    return prediction, index

# Hand landmarks connections
connections = [
    (0, 1), (1, 2), (2, 3), (3, 4),  # Thumb connections
    (0, 5), (5, 6), (6, 7), (7, 8),  # Index finger connections
    (0, 9), (9, 10), (10, 11), (11, 12),  # Middle finger connections
    (0, 13), (13, 14), (14, 15), (15, 16),  # Ring finger connections
    (0, 17), (17, 18), (18, 19), (19, 20)  # Pinky finger connections
]

# Main loop
while True:
    # Read the webcam frame
    success, img = cap.read()

    # Flip the image horizontally (mirror the camera feed)
    img = cv2.flip(img, 1)  # This line flips the camera feed

    imgOutput = img.copy()  # Copy the frame to display outputs

    # Detect hands and draw the skeleton with landmarks
    hands, img = detector.findHands(img, draw=True)  # Draw landmarks and connections

    if hands:  # If hands are detected
        if len(hands) == 2:  # Check if two hands are detected
            # Combine the bounding boxes of both hands to form a single large bounding box
            x1, y1, w1, h1 = hands[0]['bbox']
            x2, y2, w2, h2 = hands[1]['bbox']

            # Calculate the coordinates of the combined bounding box
            x_combined = min(x1, x2)
            y_combined = min(y1, y2)
            w_combined = max(x1 + w1, x2 + w2) - x_combined
            h_combined = max(y1 + h1, y2 + h2) - y_combined

            # **Remove the rectangle drawing line** for the combined bounding box
            # cv2.rectangle(imgOutput, (x_combined - offset, y_combined - offset),
            #               (x_combined + w_combined + offset, y_combined + h_combined + offset),
            #               (255, 0, 255), 4)

            # Speak "Thank you" when both hands are detected together
            current_time = time.time()
            if (current_time - last_speech_times[0] >= speech_delay) and (current_time - last_speech_times[1] >= speech_delay):
                speech_queue.put("Thank you")
                last_speech_times[0] = current_time  # Update speech time for both hands
                last_speech_times[1] = current_time  # Update speech time for both hands

        else:
            for hand_index, hand in enumerate(hands):  # If only one hand is detected, classify the gesture
                # Get the bounding box of the hand
                x, y, w, h = hand['bbox']

                # Create a white image to place the resized hand image for classification
                imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255

                # Crop the hand from the image, checking that the crop coordinates are valid
                if y - offset >= 0 and x - offset >= 0 and y + h + offset <= img.shape[0] and x + w + offset <= img.shape[1]:
                    imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]
                else:
                    imgCrop = np.zeros((1, 1, 3), np.uint8)  # Empty image if crop coordinates are invalid

                imgCropShape = imgCrop.shape

                if imgCropShape[0] == 1 and imgCropShape[1] == 1:  # Check if crop is empty
                    continue  # Skip this frame if the crop is invalid

                # Calculate the aspect ratio of the cropped hand
                aspectRatio = h / w

                # Handle aspect ratio to fit the hand in the target image size
                if aspectRatio > 1:
                    k = imgSize / h
                    wCal = math.ceil(k * w)
                    imgResize = cv2.resize(imgCrop, (wCal, imgSize))
                    imgResizeShape = imgResize.shape
                    wGap = math.ceil((imgSize - wCal) / 2)
                    imgWhite[:, wGap:wCal + wGap] = imgResize
                    # Get prediction from the classifier without output (suppress Keras logs)
                    prediction, index = predict_without_output(imgWhite)
                else:
                    k = imgSize / w
                    hCal = math.ceil(k * h)
                    imgResize = cv2.resize(imgCrop, (imgSize, hCal))
                    imgResizeShape = imgResize.shape
                    hGap = math.ceil((imgSize - hCal) / 2)
                    imgWhite[hGap:hCal + hGap, :] = imgResize
                    # Get prediction from the classifier without output (suppress Keras logs)
                    prediction, index = predict_without_output(imgWhite)

                # Draw a rectangle around the hand on the output image
                # **Remove this line if you don't want a bounding box around the individual hands**
                # cv2.rectangle(imgOutput, (x - offset, y - offset - 50),
                #               (x - offset + 90, y - offset - 50 + 50), (255, 0, 255), cv2.FILLED)

                # Display the predicted label (e.g., "Hello", "How", "I") above the bounding box
                predicted_label = labels[index]
                cv2.putText(imgOutput, predicted_label, (x, y - 26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)

                # Check if enough time has passed to speak the label again (1 second)
                current_time = time.time()  # Get current time
                if predicted_label != previous_labels[hand_index] and (current_time - last_speech_times[hand_index] >= speech_delay):
                    # Add the label to the speech queue and update speech time
                    speech_queue.put(predicted_label)
                    previous_labels[hand_index] = predicted_label
                    last_speech_times[hand_index] = current_time

    # Display the output image
    cv2.imshow("Image", imgOutput)

    # Break the loop if the user presses the 'q' key
    if cv2.waitKey(1) & 0xFF == ord('q'):
        speech_queue.put("STOP")  # Stop the speech thread
        break

# Release the webcam and close the OpenCV windows
cap.release()
cv2.destroyAllWindows()
